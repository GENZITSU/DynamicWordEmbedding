{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 時系列ベクトル作成のパイプライン作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリ・関数の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\", \"/mnt/NAS0CAC8A/collaborations/dentsuPR2019/raw_tweet/\")\n",
    "MeCab_DICT_PATH=os.getenv(\"MeCab_DICT_PATH\", \"/usr/lib/mecab/dic/mecab-ipadic-neologd/\")\n",
    "TXT_DATA_NAME=os.getenv(\"TXT_DATA_NAME\", \"tokenized_tweets\")\n",
    "PREPROCESSED_DATA_PATH = os.getenv(\"PREPROCESSED_DATA_PATH\",\n",
    "                                    \"/mnt/ssd3/k-syo/DW2V_daily/preprocessed_data/\")\n",
    "N_JOB = int(os.getenv(\"N_JOB\", \"15\"))\n",
    "WORD_FREQ_MIN = 35\n",
    "DW2V_PATH = os.getenv(\"DW2V_PATH\", \"/mnt/ssd3/k-syo/DW2V_daily/\")\n",
    "PARAM_PATH = os.getenv(\"PARAM_PATH\", \"/localHDD/k-syo/DynamicWordEmbedding/params/DW2V/\")\n",
    "\n",
    "os.environ[\"N_JOB\"] = \"15\"\n",
    "os.environ[\"WORD_FREQ_MIN\"] = \"35\"\n",
    "os.environ[\"DATA_PATH\"] = \"/mnt/NAS0CAC8A/collaborations/dentsuPR2019/raw_tweet/\"\n",
    "os.environ[\"MeCab_DICT_PATH\"] = \"/usr/lib/mecab/dic/mecab-ipadic-neologd/\"\n",
    "os.environ[\"TXT_DATA_NAME\"] = \"tokenized_tweets\"\n",
    "os.environ[\"PREPROCESSED_DATA_PATH\"] = \"/mnt/ssd3/k-syo/DW2V_daily/preprocessed_data/\"\n",
    "os.environ[\"DW2V_PATH\"] =  \"/mnt/ssd3/k-syo/DW2V_daily/\"\n",
    "os.environ[\"SLACK_URL\"] = \"https://hooks.slack.com/services/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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T04:54:32.315525Z",
     "start_time": "2019-07-13T04:54:32.267731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# jupyter 関係\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from core.utils import start_logging, timer, do_job\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\")) \n",
    "\n",
    "# カラム全表示\n",
    "pd.set_option('max_columns',None)\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "# 小数点の設定\n",
    "%precision 5\n",
    "np.random.seed(20190524)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = start_logging(filename=\"job.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ツイートデータの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 前処理\n",
    "with do_job(\"preprocess tweet\", LOGGER):\n",
    "    from core.preprocess_tweet import preprocess_one_day_tweet\n",
    "\n",
    "    TWEETS_PATHS = sorted(glob.glob(DATA_PATH+\"alldata_20*\"))\n",
    "#     TWEETS_PATHS = TWEETS_PATHS[210:]\n",
    "\n",
    "    if not os.path.exists(PREPROCESSED_DATA_PATH+\"tokenized_tweets\"):\n",
    "        os.mkdir(PREPROCESSED_DATA_PATH+\"tokenized_tweets\")\n",
    "\n",
    "    with Pool(processes=N_JOB) as p:\n",
    "        p.map(preprocess_one_day_tweet, TWEETS_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TWEETS_PATHS = glob.glob(PREPROCESSED_DATA_PATH+\"tokenized_tweets/*\")\n",
    "# TWEETS_PATHS = sorted(TWEETS_PATHS)\n",
    "\n",
    "# save_dir = \"/mnt/NAS0CAC8A/k-syo/DW2V/preprocessed_data/concated_tweets/\"\n",
    "# for i  in range(len(TWEETS_PATHS) // 7):\n",
    "#     tweets = []\n",
    "#     for tweet_path in TWEETS_PATHS[7*i:7*(i+1)]:\n",
    "#             with open(tweet_path, mode=\"rb\") as f:\n",
    "#                 tweet = pickle.load(f)\n",
    "#             tweets.append(tweet)\n",
    "#     concated_tweet = pd.concat(tweets)\n",
    "#     date = TWEETS_PATHS[7*i][-17:-7]\n",
    "#     with open(save_dir+date+\".pickle\", mode=\"wb\") as f:\n",
    "#         pickle.dump(concated_tweet, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 単語集合を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from core.make_DW2V import make_unique_word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TWEETS_PATHS = sorted(glob.glob(PREPROCESSED_DATA_PATH+\"tokenized_tweets/*\"))\n",
    "# TWEETS_PATHS = glob.glob(PREPROCESSED_DATA_PATH+\"concated_tweets/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "make_unique_word2idx(TWEETS_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 単語の共起のカウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from core.make_DW2V import make_whole_day_co_occ_dict\n",
    "from core.make_DW2V import make_one_day_co_occ_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "TWEETS_PATHS = glob.glob(PREPROCESSED_DATA_PATH+\"tokenized_tweets/*\")\n",
    "print(len(TWEETS_PATHS))\n",
    "\n",
    "# 終わったものを取り除く\n",
    "all_date = [tweet.split(\"/\")[-1][-19:-7] for tweet in TWEETS_PATHS]\n",
    "\n",
    "finished_tweets = glob.glob(PREPROCESSED_DATA_PATH+\"co_occ_dict_word_count/*\")\n",
    "finished_date = [tweet.split(\"/\")[-1][:-7] for tweet in  finished_tweets]\n",
    "\n",
    "unfinished_date = [date for date in all_date if date not in finished_date]\n",
    "\n",
    "TWEETS_PATHS = [PREPROCESSED_DATA_PATH+f\"tokenized_tweets/{date}.pickle\" for date in unfinished_date]\n",
    "print(len(TWEETS_PATHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "make_whole_day_co_occ_dict(TWEETS_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 時系列ごとにPPMIを計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from core.make_DW2V import make_whole_day_ppmi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n",
      "274\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "TWEETS_PATHS = sorted(glob.glob(PREPROCESSED_DATA_PATH+\"tokenized_tweets/*\"))\n",
    "DICTS_PATHS = sorted(glob.glob(PREPROCESSED_DATA_PATH+\"co_occ_dict_word_count/*\"))\n",
    "print(len(TWEETS_PATHS))\n",
    "\n",
    "# 終わったものを取り除く\n",
    "all_date = [tweet.split(\"/\")[-1][-19:-7] for tweet in TWEETS_PATHS]\n",
    "\n",
    "finished_tweets = glob.glob(PREPROCESSED_DATA_PATH+\"ppmi_list/*\")\n",
    "finished_date = [tweet.split(\"/\")[-1][:-7] for tweet in  finished_tweets]\n",
    "\n",
    "unfinished_date = [date for date in all_date if date not in finished_date]\n",
    "\n",
    "TWEETS_PATHS = [PREPROCESSED_DATA_PATH+f\"tokenized_tweets/{date}.pickle\" for date in unfinished_date]\n",
    "DICTS_PATHS = [PREPROCESSED_DATA_PATH+f\"co_occ_dict_word_count/{date}.pickle\" for date in unfinished_date]\n",
    "print(len(TWEETS_PATHS))\n",
    "print(len(DICTS_PATHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PATH_TUPLES = [(tweet_p, dict_p) for tweet_p, dict_p in zip(TWEETS_PATHS, DICTS_PATHS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "make_whole_day_ppmi_list(PATH_TUPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPMIから時系列embeddingを得る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from core.make_DW2V import make_DW2V\n",
    "# param_path = PARAM_PATH+\"params_1205.json\"\n",
    "# make_DW2V(param_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高速バージョン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_path = PARAM_PATH+\"params_1204.json\"\n",
    "EPS = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import util_timeCD as util\n",
    "def update_t_th_vector(t):\n",
    "    global Vlist, Ulist\n",
    "    global PPMI_PATHS, savefile\n",
    "    global embed_size, lam, gam, tau, emph\n",
    "    global LOGGER\n",
    "    f = PPMI_PATHS[t]\n",
    "    date = f[-17:-7]\n",
    "    pmi = util.getmat(f,vocab_size,False)\n",
    "    for ind in b_ind:\n",
    "        ## UPDATE U, V\n",
    "        pmi_seg = pmi[:,ind]\n",
    "        if t==0:\n",
    "            vp = np.zeros((len(ind),embed_size))\n",
    "            up = np.zeros((len(ind),embed_size))\n",
    "            iflag = 1\n",
    "        else:\n",
    "            vp = Vlist[t-1][ind]\n",
    "            up = Ulist[t-1][ind]\n",
    "            iflag = 0\n",
    "\n",
    "        if t==T-1:\n",
    "            vn = np.zeros((len(ind),embed_size))\n",
    "            un = np.zeros((len(ind),embed_size))\n",
    "            iflag = 1\n",
    "        else:\n",
    "            vn = Vlist[t+1][ind]\n",
    "            un = Ulist[t+1][ind]\n",
    "            iflag = 0\n",
    "        U_t = util.update(Ulist[t],emph*pmi_seg,vp,vn,\n",
    "                                    lam,tau,gam,ind,embed_size,iflag)\n",
    "        V_t = util.update(Vlist[t],emph*pmi_seg,up,un,\n",
    "                                    lam,tau,gam,ind,embed_size,iflag)\n",
    "        with open(f\"{savefile}/tmp_V/{date}.pickle\", mode=\"wb\") as f:\n",
    "            pickle.dump(V_t, f, protocol=-1)\n",
    "        with open(f\"{savefile}/tmp_U/{date}.pickle\", mode=\"wb\") as f:\n",
    "            pickle.dump(U_t, f, protocol=-1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oepn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5034530d894e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0moepn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{savefile}diffs.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'oepn' is not defined"
     ]
    }
   ],
   "source": [
    "# PPMIのパスを読み込む\n",
    "PPMI_PATHS = sorted(glob.glob(PREPROCESSED_DATA_PATH+\"ppmi_list/*\"))\n",
    "# number of time spans\n",
    "T = len(PPMI_PATHS)\n",
    "\n",
    "## PARAMETERS\n",
    "params = json.load(open(param_path, mode=\"r\"))\n",
    "ITERS = params[\"ITERS\"]\n",
    "embed_size  = params[\"embed_size\"]\n",
    "lam = params[\"lam\"] # weight decay\n",
    "gam = params[\"gam\"] # forcing regularizer\n",
    "tau = params[\"tau\"]  # smoothing regularizer\n",
    "emph = params[\"emph\"] # emphasize value will not be zero\n",
    "\n",
    "\n",
    "# 保存先の確保\n",
    "savefile = DW2V_PATH+\"Lam_\"+str(lam)+\"_Tau_\"+str(tau)+\"_Gam_\"+str(gam)\\\n",
    "                                        +\"_Dim_\"+str(embed_size)+\"_A_\"+str(emph)\n",
    "if not os.path.exists(savefile):\n",
    "        os.mkdir(savefile)\n",
    "if not os.path.exists(f\"{savefile}/tmp_V/\"):\n",
    "        os.mkdir(f\"{savefile}/tmp_V/\")\n",
    "if not os.path.exists(f\"{savefile}/tmp_U/\"):\n",
    "        os.mkdir(f\"{savefile}/tmp_U/\")\n",
    "\n",
    "# ベクトルの初期化\n",
    "with open(PREPROCESSED_DATA_PATH+\"filtered_word2idx.pickle\", mode=\"rb\") as f:\n",
    "    filtered_word2idx = pickle.load(f)\n",
    "vocab_size = len(filtered_word2idx.keys())\n",
    "del filtered_word2idx\n",
    "batch_size = vocab_size\n",
    "b_ind= util.getbatches(vocab_size, batch_size)\n",
    "\n",
    "# 学習開始\n",
    "diffs = []\n",
    "for iteration in range(ITERS):\n",
    "    with do_job(f\"iter {iteration+1} / {ITERS}\", LOGGER):\n",
    "        # 1 epoch前のベクトルを読み出す\n",
    "        try:\n",
    "            Ulist = pickle.load(open(f\"{savefile}/ngU_iter{iteration-1}.pickle\", mode=\"rb\" ))\n",
    "            Vlist = pickle.load(open(f\"{savefile}/ngV_iter{iteration-1}.pickle\", mode=\"rb\" ))\n",
    "        except(IOError):\n",
    "            Ulist,Vlist = util.initvars(vocab_size, T, embed_size)\n",
    "\n",
    "        # 更新\n",
    "        times = np.arange(T)\n",
    "        with Pool(processes=10) as p:\n",
    "            p.map(update_t_th_vector, times)\n",
    "        for t, ppmi_path in enumerate(PPMI_PATHS):\n",
    "            date = ppmi_path[-17:-7]\n",
    "            with open(f\"{savefile}/tmp_V/{date}.pickle\", mode=\"rb\") as f:\n",
    "                Vlist[t] = pickle.load(f)\n",
    "            with open(f\"{savefile}/tmp_U/{date}.pickle\", mode=\"rb\") as f:\n",
    "                Ulist[t] = pickle.load(f)\n",
    "\n",
    "        # 保存\n",
    "        pickle.dump(Ulist, open(f\"{savefile}/ngU_iter{iteration}.pickle\", mode=\"wb\"), protocol=-1)\n",
    "        pickle.dump(Vlist, open(f\"{savefile}/ngV_iter{iteration}.pickle\", mode=\"wb\"), protocol=-1)\n",
    "\n",
    "        if iteration >= 2:\n",
    "            # HDDの節約\n",
    "            os.remove(f\"{savefile}/ngU_iter{iteration-2}.pickle\")\n",
    "            os.remove(f\"{savefile}/ngV_iter{iteration-2}.pickle\")\n",
    "\n",
    "        diff_U, diff_V, diff_U_V = util.check_diff(iteration, savefile)\n",
    "        diffs.append([diff_U, diff_V, diff_U_V])\n",
    "\n",
    "    # ほとんど変化しなくなったら終了\n",
    "    LOGGER.info(f\"diff_U: {diff_U}\\n diff_V: {diff_V}\")\n",
    "    if min(diff_U, diff_V/2) < EPS and diff_U != 0.:\n",
    "        break\n",
    "\n",
    "with oepn(f\"{savefile}diffs.pickle\", mode=\"wb\") as f:\n",
    "    pickle.dump(diffs, f, protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "336px",
    "left": "1004px",
    "right": "20px",
    "top": "148px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
